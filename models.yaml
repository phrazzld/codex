# Maps provider names to environment variable names containing API keys
api_key_sources:
  openai: "OPENAI_API_KEY"
  gemini: "GEMINI_API_KEY"
  openrouter: "OPENROUTER_API_KEY"

# Providers
# ---------
# Defines available LLM service providers
providers:
  - name: openai
    # Uncomment to use a custom API endpoint:
    # base_url: "https://your-openai-proxy.example.com/v1"

  - name: gemini
    # Uncomment to use a custom API endpoint:
    # base_url: "https://your-gemini-proxy.example.com/v1"

  - name: openrouter
    # Default API endpoint is https://openrouter.ai/api/v1
    # Uncomment to use a custom API endpoint:
    # base_url: "https://your-openrouter-proxy.example.com/api/v1"

# Models
# ------
# Defines available LLM models with their capabilities and parameters
models:
  # OpenAI Models
  # -------------

  - name: gpt-4.1
    provider: openai
    api_model_id: gpt-4.1
    context_window: 1000000
    max_output_tokens: 200000
    parameters:
      temperature:
        type: float
        default: 0.7
      top_p:
        type: float
        default: 1.0
      frequency_penalty:
        type: float
        default: 0.0
      presence_penalty:
        type: float
        default: 0.0

  - name: o4-mini
    provider: openai
    api_model_id: o4-mini
    context_window: 200000
    max_output_tokens: 200000
    parameters:
      temperature:
        type: float
        default: 1.0
      top_p:
        type: float
        default: 1.0
      frequency_penalty:
        type: float
        default: 0.0
      presence_penalty:
        type: float
        default: 0.0
      reasoning:
        type: object
        default:
          effort: "high"

  - name: o3
    provider: openai
    api_model_id: o3
    context_window: 200000
    max_output_tokens: 100000
    parameters:
      temperature:
        type: float
        default: 1.0
      top_p:
        type: float
        default: 1.0
      frequency_penalty:
        type: float
        default: 0.0
      presence_penalty:
        type: float
        default: 0.0
      reasoning:
        type: object
        default:
          effort: "high"

  # Gemini Models
  # -------------
  - name: gemini-2.5-pro
    provider: gemini
    api_model_id: gemini-2.5-pro
    context_window: 1048576
    max_output_tokens: 65536
    parameters:
      temperature:
        type: float
        default: 0.7
      top_p:
        type: float
        default: 0.95
      top_k:
        type: int
        default: 40

  - name: gemini-2.5-flash
    provider: gemini
    api_model_id: gemini-2.5-flash
    context_window: 1048576
    max_output_tokens: 65536
    parameters:
      temperature:
        type: float
        default: 0.7
      top_p:
        type: float
        default: 0.95
      top_k:
        type: int
        default: 40

  # OpenRouter Models
  # ----------------
  # OpenRouter provides a unified gateway to access models from various providers
  # Model IDs use the format: provider/model-name

  - name: openrouter/deepseek/deepseek-chat-v3-0324
    provider: openrouter
    api_model_id: deepseek/deepseek-chat-v3-0324
    context_window: 65536  # 64k tokens
    max_output_tokens: 8192
    parameters:
      temperature:
        type: float
        default: 0.7
      top_p:
        type: float
        default: 0.95

  - name: openrouter/deepseek/deepseek-prover-v2
    provider: openrouter
    api_model_id: deepseek/deepseek-prover-v2
    context_window: 163840
    max_output_tokens: 163840
    parameters:
      temperature:
        type: float
        default: 0.7
      top_p:
        type: float
        default: 0.95

  - name: openrouter/deepseek/deepseek-r1-0528
    provider: openrouter
    api_model_id: deepseek/deepseek-r1-0528
    context_window: 131072  # 128k tokens
    max_output_tokens: 33792
    parameters:
      temperature:
        type: float
        default: 0.7
      top_p:
        type: float
        default: 0.95

  - name: openrouter/x-ai/grok-3-beta
    provider: openrouter
    api_model_id: x-ai/grok-3-beta
    context_window: 131072  # 131k tokens
    max_output_tokens: 131072
    parameters:
      temperature:
        type: float
        default: 0.7
      top_p:
        type: float
        default: 0.95

  - name: openrouter/x-ai/grok-3-mini-beta
    provider: openrouter
    api_model_id: x-ai/grok-3-mini-beta
    context_window: 131072  # 131k tokens
    max_output_tokens: 131072
    parameters:
      reasoning:
        type: object
        default:
          effort: "high"
      temperature:
        type: float
        default: 1.0
      top_p:
        type: float
        default: 0.95

  - name: openrouter/meta-llama/llama-4-maverick
    provider: openrouter
    api_model_id: meta-llama/llama-4-maverick
    context_window: 1048576
    max_output_tokens: 1048576
    parameters:
      temperature:
        type: float
        default: 0.7
      top_p:
        type: float
        default: 0.95

  - name: openrouter/meta-llama/llama-4-scout
    provider: openrouter
    api_model_id: meta-llama/llama-4-scout
    context_window: 1048576
    max_output_tokens: 1048576
    parameters:
      temperature:
        type: float
        default: 0.7
      top_p:
        type: float
        default: 0.95

